{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLMOPS\n",
    "1. Load Data / Processing / Embeddings\n",
    "2. Model Selection / Load LLM / Fine-Tunning / Prompt Engineering\n",
    "3. LLM Chain and Agents\n",
    "4. Evaluate / Metrics\n",
    "5. Deployment / API Gateaway\n",
    "6. Monitoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data / Pre-Processing / Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.document_loaders import PDFPlumberLoader\n",
    "\n",
    "#File path with the PDF that I will use for testing\n",
    "file_path = r'documents\\SIGNED CompanyB_SOW_Tableau Resouces IT Q2-Q4 2023_1 April 2024 1.pdf'\n",
    "file_name = os.path.splitext(file_path)[0].split('/')[-1]\n",
    "\n",
    "loader = PDFPlumberLoader(file_path)\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=300,\n",
    "    chunk_overlap=30,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    "    )\n",
    "\n",
    "chunks = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "model_name = 'BAAI/bge-large-en-v1.5'\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "vectordb = FAISS.from_documents(documents = chunks, embedding = embedding_model)\n",
    "vectordb.save_local(f'embeddings'/{file_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load LLM / Prompt Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Selection and Load "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders.base import BaseLoader\n",
    "from langchain_core.callbacks import CallbackManagerForLLMRun\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.language_models.llms import BaseLLM\n",
    "from langchain_core.outputs import Generation, LLMResult\n",
    "from langchain_core.pydantic_v1 import Field, root_validator\n",
    "from typing import Any, Dict, Iterator, List, Optional\n",
    "import requests\n",
    "\n",
    "class CustomLLM(BaseLLM):\n",
    "    def __init__(self):\n",
    "        super(CustomLLM, self).__init__()\n",
    "        self.callbacks=None\n",
    "        self.verbose=False\n",
    "        self.tags=None\n",
    "        self.metadata = None\n",
    "        self.cache= None\n",
    "\n",
    "    @property\n",
    "    def _default_params(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get the default parameters for calling vllm.\"\"\"\n",
    "        return {}\n",
    "\n",
    "    @root_validator(allow_reuse=True)\n",
    "    def validate_environment(cls, values: Dict) -> Dict:\n",
    "        \"\"\"Validate that python package exists in environment.\"\"\"\n",
    "\n",
    "        return {}\n",
    "    \n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        \"\"\"Return type of llm.\"\"\"\n",
    "        return \"vllm\"\n",
    "\n",
    "    def _generate(\n",
    "        self,\n",
    "        prompts: List[str],\n",
    "        params: dict ={'temperature':0.7, 'top_p':1.0, 'max_tokens':200, 'skip_special_tokens':True,'stop':['Note','Please'],'frequency_penalty':1.2},        \n",
    "        **kwargs: Any,\n",
    "    ) -> LLMResult:\n",
    "        \"\"\"Run the LLM on the given prompt and input.\"\"\"\n",
    "        \n",
    "        # call the model\n",
    "        response = requests.post(\n",
    "            \"http://3.108.133.55:8502/llama_generate\",\n",
    "            json={\n",
    "                \"prompt\": prompts,\n",
    "                \"kwargs\":params\n",
    "            },\n",
    "        )\n",
    "        outputs = response.json()\n",
    "  \n",
    "        generations = []\n",
    "        for output in outputs:\n",
    "            text = output['outputs'][0]['text']\n",
    "            generations.append([Generation(text=text)])\n",
    "\n",
    "        return LLMResult(generations=generations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = CustomLLM()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
    "\n",
    "Question: {question} \n",
    "\n",
    "Context: {context} \n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"question\", \"context\"], template=template\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.chains import LLMChain, RetrievalQA\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "llm = CustomLLM()\n",
    "#This was the query used to extract the context needed\n",
    "\n",
    "# Retrieve and generate using the relevant snippets of the blog.\n",
    "retriever = vectordb.as_retriever()\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chatbot(question:str)->str:\n",
    "    answer=rag_chain.invoke(question)\n",
    "    print(answer)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Based on the provided context, the companies involved in this contract are:\n",
      "\n",
      "1. Company C LLP\n",
      "2. Company B Limited\n"
     ]
    }
   ],
   "source": [
    "response=chatbot(\"Which are the companies between this contract?\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e766a111639de11b5869a13efc19c4853cf3ac9e9cd79f07bc4672b76b934792"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
