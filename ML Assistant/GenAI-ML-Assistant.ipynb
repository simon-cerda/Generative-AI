{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "from langchain_core.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import NotebookLoader\n",
    "\n",
    "loader = NotebookLoader(\n",
    "    \"stock-price-prediction-using-xgboost-prophet-arima.ipynb\",\n",
    "    include_outputs=True,\n",
    "    max_output_length=50,\n",
    "    remove_newline=False,\n",
    ")\n",
    "\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Azure GPT4 LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "import os\n",
    "llm = AzureChatOpenAI(\n",
    "            temperature=0,\n",
    "            deployment_name=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n",
    "            azure_endpoint=os.environ[\"AZURE_OPENAI_API_BASE\"],\n",
    "            openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n",
    "            openai_api_key=os.environ[\"AZURE_OPENAI_API_KEY\"],\n",
    "\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = documents[0].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing and Feature Engineering:\n",
      "1. AAPL stock data is downloaded using the yfinance library.\n",
      "2. Only the last 3 years of stock data are used, and all columns except 'Close' are dropped.\n",
      "3. Lag features (lag1 to lag12) are created by shifting the 'Close' value by multiples of the prediction horizon (30 days).\n",
      "4. Time-based features are created, including hour, day of the week, quarter, month, year, day of the year, day of the month, and week of the year.\n",
      "\n",
      "Training Model - XGBoost:\n",
      "1. The dataset is prepared for the XGBoost model by applying the feature engineering functions.\n",
      "2. The dataset is split into training and test sets using a 70-30 split.\n",
      "3. Optuna is used for hyperparameter optimization to find the best parameters for the XGBoost model.\n",
      "4. The best parameters are used to train the final XGBoost model.\n",
      "5. The model is evaluated on the test set, and the Root Mean Squared Error (RMSE) is reported.\n",
      "\n",
      "Evaluation - XGBoost:\n",
      "1. The Mean Absolute Percentage Error (MAPE) is calculated for the XGBoost model predictions on the test set.\n",
      "2. Feature importances are extracted from the trained XGBoost model to understand which features are most influential.\n",
      "\n",
      "Training Model - Prophet:\n",
      "1. The stock data is split into training and test sets with an 80-20 split.\n",
      "2. The training data is preprocessed to fit the Prophet model's requirements (renaming columns to 'ds' and 'y').\n",
      "3. The Prophet model is trained on the preprocessed training data.\n",
      "\n",
      "Evaluation - Prophet:\n",
      "1. The test data is preprocessed similarly to the training data.\n",
      "2. Predictions are made on the test set using the trained Prophet model.\n",
      "3. The MAPE is calculated for the Prophet model predictions on the test set.\n",
      "\n",
      "Training Model - ARIMA:\n",
      "1. The stock data is split into training and test sets with an 80-20 split.\n",
      "2. The auto_arima function from the pmdarima library is used to perform hyperparameter tuning and find the best ARIMA model parameters.\n",
      "3. The ARIMA model is trained using the best parameters found.\n",
      "\n",
      "Evaluation - ARIMA:\n",
      "1. Predictions are made on the test set using the trained ARIMA model.\n",
      "2. The MAPE is calculated for the ARIMA model predictions on the test set.\n",
      "\n",
      "Final Comparison:\n",
      "1. The accuracies of the XGBoost, ARIMA, and Prophet models are compared by subtracting the MAPE from 100% to get the accuracy percentage.\n",
      "2. The XGBoost model has the highest accuracy, followed by ARIMA, and then Prophet.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "prompt_template = \"\"\"Below you have the content of a notebook with a machine learning process, I need you to give me a step by step concise summary of the content.\n",
    "Focus on Pre Processing, Feature Engineering,Training Model and Evaluation.\n",
    "\"{text}\"\n",
    "CONCISE SUMMARY:\"\"\"\n",
    "PROMPT = PromptTemplate(template=prompt_template, input_variables=[\"text\"])\n",
    "\n",
    "chain = LLMChain(llm=llm,prompt=PROMPT)\n",
    "summary=chain.invoke(text)\n",
    "print(summary['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QnA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "#Using all the content as context to answer question\n",
    "notebook_content = documents[0].page_content\n",
    "\n",
    "prompt = (\n",
    "    PromptTemplate.from_template(\"You are an expert Machine Learning Engineer and Data Scientist, analyze the following ML pipeline notebook to answer the provided question.\")\n",
    "+f\"\"\"Notebook: {notebook_content.replace('{','(').replace('}',')')} \\n\\n \"\"\"\n",
    "    + \"Question: {question} \\n\"\n",
    "    + \"Helpful Answer: \\n\"\n",
    ")\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The models tested in your pipeline are:\n",
      "\n",
      "1. XGBoost (Extreme Gradient Boosting)\n",
      "2. Prophet (by Facebook)\n",
      "3. ARIMA (AutoRegressive Integrated Moving Average)\n"
     ]
    }
   ],
   "source": [
    "response=chain.invoke({\"question\":\"which are all the model tested on my pipeline?\"})\n",
    "print(response['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The feature engineering steps taken to build the model include:\n",
      "\n",
      "1. **Lag Features**: The code creates lag features for the 'Close' column of the stock data. Specifically, it creates 12 lag features, each shifted by a multiple of the `num_days_pred` (30 days). These lag features are intended to capture the temporal dependencies in the stock prices.\n",
      "\n",
      "2. **Time-based Features**: The code also creates time-based features from the index of the DataFrame, which is a DateTimeIndex. These features include:\n",
      "   - `hour`: The hour of the timestamp (though this is not relevant for daily stock data and remains 0).\n",
      "   - `dayofweek`: The day of the week.\n",
      "   - `quarter`: The quarter of the year.\n",
      "   - `month`: The month of the year.\n",
      "   - `year`: The year.\n",
      "   - `dayofyear`: The day of the year.\n",
      "   - `dayofmonth`: The day of the month.\n",
      "   - `weekofyear`: The week of the year.\n",
      "\n",
      "Recommendations to improve feature engineering:\n",
      "\n",
      "1. **Remove Irrelevant Features**: Since the stock data is daily, the `hour` feature is not relevant and can be removed to reduce noise in the model.\n",
      "\n",
      "2. **Incorporate External Data**: Stock prices can be influenced by various factors, including economic indicators, company news, and market sentiment. Incorporating such external data could potentially improve the model's predictive power.\n",
      "\n",
      "3. **Feature Selection**: The feature importance output shows that some features like `year` and certain lags are more important than others. It might be beneficial to perform feature selection to keep only the most relevant features.\n",
      "\n",
      "4. **Domain-Specific Features**: Adding features that are specific to stock market analysis, such as moving averages, exponential moving averages, Bollinger Bands, Relative Strength Index (RSI), and others, could capture more aspects of the stock's behavior.\n",
      "\n",
      "5. **Dimensionality Reduction**: Techniques like PCA (Principal Component Analysis) could be used to reduce the dimensionality of the feature space if it becomes too large after adding more features.\n",
      "\n",
      "6. **Regularization**: To prevent overfitting, especially when adding more features, consider using regularization techniques in the model training process.\n",
      "\n",
      "7. **High-Frequency Data**: If higher-frequency data (e.g., hourly or minute-level data) is available, it could be used to capture more granular patterns in stock price movements.\n",
      "\n",
      "8. **Seasonality and Trend Decomposition**: For time series data, it can be useful to decompose the series into trend, seasonal, and residual components and use these components as features.\n",
      "\n",
      "9. **Sentiment Analysis**: Sentiment analysis on news articles or social media posts related to AAPL could provide insights into public sentiment, which can be a powerful driver of stock prices.\n",
      "\n",
      "10. **Volume Data**: Trading volume is an important indicator in stock markets and can be used as a feature to improve the model.\n",
      "\n",
      "By refining the feature engineering process and incorporating additional relevant data, the predictive performance of the machine learning models could be further enhanced.\n"
     ]
    }
   ],
   "source": [
    "response=chain.invoke({\"question\":'Which Feature Engineering where made to build the model, do you have any recommendation to improve it?'})\n",
    "\n",
    "print(response['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given the context of the notebook, which is focused on time series forecasting for AAPL stock prices, you have already experimented with XGBoost, Prophet, and ARIMA models. Each of these models has its strengths and weaknesses, and the choice of additional models to test would depend on the specific characteristics of the data and the goals of the forecasting task.\n",
      "\n",
      "Here are a few other machine learning and statistical models that you could consider testing:\n",
      "\n",
      "1. **Long Short-Term Memory (LSTM) Networks**: LSTMs are a type of recurrent neural network (RNN) that are well-suited for sequence prediction problems like time series forecasting. They can capture long-term dependencies in time series data, which might be beneficial for stock price predictions.\n",
      "\n",
      "2. **Gated Recurrent Units (GRUs)**: GRUs are another type of RNN that are similar to LSTMs but with a simpler structure. They can also be effective for time series forecasting and might be faster to train than LSTMs.\n",
      "\n",
      "3. **Temporal Convolutional Networks (TCNs)**: TCNs are a type of convolutional neural network (CNN) designed for sequence modeling tasks. They can handle very long sequences and have been shown to perform well on a variety of time series forecasting problems.\n",
      "\n",
      "4. **Vector Autoregression (VAR)**: VAR is a statistical model that captures the linear interdependencies among multiple time series. If you have access to additional relevant time series data (e.g., economic indicators, other stock prices), VAR could be used to model the relationships between them and AAPL stock prices.\n",
      "\n",
      "5. **Ensemble Methods**: Combining the predictions from multiple models can sometimes lead to better performance than any single model. You could create an ensemble of the models you have already trained, along with any new models you test, to see if it improves the forecasting accuracy.\n",
      "\n",
      "6. **Facebook's NeuralProphet**: NeuralProphet is an extension of the Prophet model that incorporates neural network components for improved forecasting performance. It combines the interpretable components of Prophet with the flexibility of neural networks.\n",
      "\n",
      "7. **Support Vector Regression (SVR)**: SVR is a type of support vector machine that can be used for regression tasks. It might be useful if the stock price data has a non-linear pattern that other models are not capturing well.\n",
      "\n",
      "8. **Random Forest Regressor**: Random forests are an ensemble learning method that can be used for regression. They are robust to overfitting and can capture complex interactions between features.\n",
      "\n",
      "When testing new models, it's important to consider the computational cost, the complexity of the model, and the interpretability of the results. Additionally, you should ensure that you have a robust validation strategy to assess the performance of each model accurately.\n"
     ]
    }
   ],
   "source": [
    "response=chain.invoke({\"question\":'Considering all the ML process, which other ML Model should I test?'})\n",
    "\n",
    "print(response['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The feature importance from your XGBoost model indicates how much each feature contributes to the model's predictions. The importance is calculated for each feature by the amount it increases the prediction accuracy or decreases impurity in the model. In XGBoost, feature importance is typically measured by the \"gain\", \"cover\", or \"frequency\" of the features when building the trees.\n",
      "\n",
      "From the output provided in your notebook, the feature importances are sorted in descending order, with the most important features at the top. Here's a breakdown of the top features and their relative importance:\n",
      "\n",
      "1. `year`: This feature has the highest importance with a score of approximately 0.508546. This suggests that the year of the stock data is the most significant predictor in your model. It could be capturing long-term trends in the stock price.\n",
      "\n",
      "2. `lag9`: The second most important feature is `lag9` with a score of about 0.247954. This feature represents the stock's closing price shifted by 9 times the prediction horizon (in this case, 30 days). It indicates that the stock price 270 days ago is a strong predictor of the current price.\n",
      "\n",
      "3. `lag12`: The third feature in importance is `lag12` with a score of 0.079842. Similar to `lag9`, this feature is the stock's closing price shifted by 12 times the prediction horizon, which means it looks back 360 days. It also plays a significant role in the model's predictions.\n",
      "\n",
      "4. `lag8`: With a score of 0.046287, `lag8` is the fourth most important feature. It represents the stock's closing price shifted by 240 days.\n",
      "\n",
      "5. `lag1`: The fifth feature is `lag1` with a score of 0.033997. This is the stock's closing price shifted by 30 days, indicating that the price one month ago is somewhat predictive of the current price.\n",
      "\n",
      "The remaining features have lower importance scores, indicating they have less impact on the model's predictions. It's worth noting that the features with the highest importance are all lag features, which are derived from the historical closing prices at different time steps in the past. This suggests that the model heavily relies on past stock prices to predict future prices.\n",
      "\n",
      "The `year` feature's high importance could be due to the model capturing some form of trend or structural change in the stock price that is associated with the passage of years. However, it's unusual for the `year` to be the most important feature in a stock price prediction model, as it doesn't usually change on a daily basis like stock prices do. This could be an indication of overfitting to the particular years in the training data, or it might be capturing some macroeconomic trends that are not directly related to the stock's daily movements.\n",
      "\n",
      "In summary, the most impactful features in your XGBoost model are historical lag features that capture the stock's closing price at various points in the past, with the `year` feature being the most significant. This indicates that the model is using these historical prices to infer patterns and make predictions about future stock prices.\n"
     ]
    }
   ],
   "source": [
    "response=chain.invoke({\"question\":'Explain the feature importance of my XGBoost Model, and which of them are impacting the most in my predictions'})\n",
    "\n",
    "print(response['text'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
